---
---

@string{aps = {American Physical Society,}}

@article{baankestad2024flexible,
  bibtex_show={true},
  html={https://github.com/mariabankestad/SE2-GNN},
  abstract={This paper presents a novel approach for constructing graph neural networks equivariant to 2D rotations and translations and leveraging them as PDE surrogates on non-gridded domains. We show that aligning the representations with the principal axis allows us to sidestep many constraints while preserving SE(2) equivariance. By applying our model as a surrogate for fluid flow simulations and conducting thorough benchmarks against non-equivariant models, we demonstrate significant gains in terms of both data efficiency and accuracy.},
  title={Flexible SE(2) graph neural networks with applications to PDE surrogates},
  author={B{\aa}nkestad, Maria and Mogren, Olof and Pirinen, Aleksis},
  journal={arXiv preprint arXiv:2405.20287},
  preview={sim_obs.png},
  year={2024}
}

@article{baankestad2023carbohydrate,  
bibtex_show={true},
 html={https://github.com/mariabankestad/GeqShift},
 abstract={Carbohydrates, vital components of biological systems, are well-known for their structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy plays a crucial role in understanding their intricate molecular arrangements and is essential in assessing and verifying the molecular structure of organic molecules. An important part of this process is to predict the NMR chemical shift from the molecular structure. This work introduces a novel approach that leverages E(3) equivariant graph neural networks to predict carbohydrate NMR spectra. Notably, our model achieves a substantial reduction in mean absolute error, up to threefold, compared to traditional models that rely solely on two-dimensional molecular structure. Even with limited data, the model excels, highlighting its robustness and generalization capabilities. The implications are far-reaching and go beyond an advanced understanding of carbohydrate structures and spectral interpretation. For example, it could accelerate research in pharmaceutical applications, biochemistry, and structural biology, offering a faster and more reliable analysis of molecular structures. Furthermore, our approach is a key step towards a new data-driven era in spectroscopy, potentially influencing spectroscopic techniques beyond NMR.},
  title={Carbohydrate NMR chemical shift predictions using E(3) equivariant graph neural networks},
  author={B{\aa}nkestad, Maria and Dorst, Keven M and Widmalm, G{\"o}ran and R{\"o}nnols, Jerk},
  journal={arXiv preprint arXiv:2311.12657},
  preview={drawing_small.png},
  year={2023}
}

@article{baankestad2024ising,
bibtex_show={true},
  title={Ising on the Graph: Task-specific Graph Subsampling via the Ising Model},
  author={B{\aa}nkestad, Maria and Andersson, Jennifer and Mair, Sebastian and Sj{\"o}lund, Jens},
  journal={arXiv preprint arXiv:2402.10206},
  year={2024},
  abstract = {Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.},
  preview = {ising_small.png}
}

@article{baankestad2023variational,
bibtex_show={true},
abstract = {We present elliptical processesâ€”a family of non-parametric probabilistic models that subsumes Gaussian processes and Student's t processes. This generalization includes a range of new heavy-tailed behaviors while retaining computational tractability. Elliptical processes are based on a representation of elliptical distributions as a continuous mixture of Gaussian distributions. We parameterize this mixture distribution as a spline normalizing flow, which we train using variational inference. The proposed form of the variational posterior enables a sparse variational elliptical process applicable to large-scale problems. We highlight advantages compared to Gaussian processes through regression and classification experiments. Elliptical processes can supersede Gaussian processes in several settings, including cases where the likelihood is non-Gaussian or when accurate tail modeling is essential.},
title={Variational Elliptical Processes},
author={Maria Margareta B{\r{a}}nkestad and Jens Sj{\"o}lund and Jalil Taghia and Thomas B. Sch{\"o}n},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=djN3TaqbdA},
html={https://openreview.net/forum?id=djN3TaqbdA},
preview = {elliptical.png},
}

@article{sjolund2022graph,
bibtex_show={true},
  title={Graph-based neural acceleration for nonnegative matrix factorization},
  author={Sj{\"o}lund, Jens and B{\aa}nkestad, Maria},
  journal={arXiv preprint arXiv:2202.00264},
  year={2022},
  abstract = {We describe a graph-based neural acceleration technique for nonnegative matrix factorization that builds upon a connection between matrices and bipartite graphs that is well-known in certain fields, e.g., sparse linear algebra, but has not yet been exploited to design graph neural networks for matrix computations. We first consider low-rank factorization more broadly and propose a graph representation of the problem suited for graph neural networks. Then, we focus on the task of nonnegative matrix factorization and propose a graph neural network that interleaves bipartite self-attention layers with updates based on the alternating direction method of multipliers. Our empirical evaluation on synthetic and two real-world datasets shows that we attain substantial acceleration, even though we only train in an unsupervised fashion on smaller synthetic instances.},
  html = {https://arxiv.org/abs/2202.00264},
  preview = {nmf.png},
}